{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPz0OQUUUiXC"
   },
   "source": [
    "## Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f_EyP7-qUiXE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711134416129,
     "user_tz": -60,
     "elapsed": 6454,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-03-22T16:01:50.516120Z",
     "start_time": "2024-03-22T16:01:47.460352Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, Subset\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If working on a local machine:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "LQBIpmTTvBVf"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File not found.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/train/tokenized_train_data.pkl'):\n",
    "    data = pd.read_pickle('data/train/tokenized_train_data.pkl')\n",
    "else:\n",
    "    print(\"File not found.\")"
   ],
   "metadata": {
    "id": "pKWZRnQOUiXF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711133524243,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "bb2f4ea7-12ff-49b4-bc28-ccd9adfdbc58",
    "ExecuteTime": {
     "end_time": "2024-03-22T16:02:20.572612Z",
     "start_time": "2024-03-22T16:01:56.646007Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "If working in Google Colab:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "UXqVC-W-UiXF"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_path = '/content/drive/My Drive/'\n",
    "data_path = os.path.join(base_path, 'NLP/hate_speech_detection_pipeline/data/train/tokenized_train_balanced_data.pkl')\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    data = pd.read_pickle(data_path)\n",
    "else:\n",
    "    print(\"File not found. Please ensure the file path is correct and run the previous cell to create the file.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVwHfJkWUiXF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711134437540,
     "user_tz": -60,
     "elapsed": 21414,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "ac478745-bc3a-4257-8307-4cd64c50b0be"
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "len(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4OXett1zwHC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711134437540,
     "user_tz": -60,
     "elapsed": 13,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "5845ba45-1189-4ca4-a007-2d00e69f7186",
    "ExecuteTime": {
     "end_time": "2024-03-22T16:02:24.783447Z",
     "start_time": "2024-03-22T16:02:24.771963Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32450"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initalize BERT Model"
   ],
   "metadata": {
    "id": "FstuoYooUiXG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "s31lTiclUiXG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:47.798275Z",
     "start_time": "2024-03-21T13:48:47.600170Z"
    },
    "id": "Mvkl66FXUiXG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711134437540,
     "user_tz": -60,
     "elapsed": 11,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    }
   },
   "outputs": [],
   "source": [
    "# First, we need to drop any rows that failed to tokenize\n",
    "data = data.dropna(subset=['tokenized'])\n",
    "\n",
    "# Extract 'input_ids' and 'attention_mask' and create tensors\n",
    "input_ids = torch.cat(data['tokenized'].apply(lambda x: x['input_ids']).tolist())\n",
    "attention_mask = torch.cat(data['tokenized'].apply(lambda x: x['attention_mask']).tolist())\n",
    "labels = torch.tensor(data['hate'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if CUDA is available, otherwise use CPU"
   ],
   "metadata": {
    "collapsed": false,
    "id": "BEVIpKqPUiXH"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda for training\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:50.600905Z",
     "start_time": "2024-03-21T13:48:50.596401Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoAm--8ZUiXH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711134438062,
     "user_tz": -60,
     "elapsed": 532,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "f896086f-e2d3-4396-d676-9e4d74a36c1d"
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a DataLoader for the training and validation sets"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9l3-XSzaUiXH"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `input_ids`, `attention_mask`, and `labels` are your dataset's features\n",
    "# and labels, respectively, and are all PyTorch tensors.\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:57.444297Z",
     "start_time": "2024-03-21T13:48:57.438683Z"
    },
    "id": "3Ogtd9XTUiXH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711134438062,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the model and optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Y2mA5wUSUiXH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop"
   ],
   "metadata": {
    "collapsed": false,
    "id": "-M9p96pTUiXH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the number of epochs for training\n",
    "num_epochs = 2\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model = model.to(device)  # Move the model to the appropriate device (CPU or GPU)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jT3pf2qj8Iq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711135280929,
     "user_tz": -60,
     "elapsed": 818,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "2cf49df5-1657-42ce-8650-2060382f57b2"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Listen zur Speicherung der Verlustwerte\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "\n",
    "        optimizer.zero_grad() # Gradients will set to zero after itteration\n",
    "\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "        loss = outputs.loss # Lossfunction\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping - avoiding to big gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_loss_values.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            batch_predictions = np.argmax(logits, axis=1).flatten()\n",
    "\n",
    "            # Speichern der Vorhersagen und tatsächlichen Labels für später\n",
    "            all_predictions.extend(batch_predictions)\n",
    "            all_true_labels.extend(label_ids)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    val_loss_values.append(avg_val_loss)\n",
    "\n",
    "    # F1-Score für das gesamte Validierungsset berechnen\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='macro', zero_division=0)\n",
    "    val_f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation F1 Score: {f1:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URZoTWSMiULh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711136976619,
     "user_tz": -60,
     "elapsed": 1692801,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "a173cd4d-da21-4435-af00-6fc402306d49"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training Epoch 1: 100%|██████████| 1826/1826 [13:36<00:00,  2.24it/s]\n",
      "Validation Epoch 1: 100%|██████████| 203/203 [00:30<00:00,  6.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - Train Loss: 0.2055, Validation Loss: 0.1882, Validation F1 Score: 0.9303\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training Epoch 2: 100%|██████████| 1826/1826 [13:35<00:00,  2.24it/s]\n",
      "Validation Epoch 2: 100%|██████████| 203/203 [00:30<00:00,  6.65it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 - Train Loss: 0.1096, Validation Loss: 0.2299, Validation F1 Score: 0.9405\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Erstellen Sie eine Figur und ein Subplot-Grid von 1x2\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# Erster Plot: Verlust-Lernkurve\n",
    "ax1.plot(train_loss_values, label='Trainingsverlust')\n",
    "ax1.plot(val_loss_values, label='Validierungsverlust')\n",
    "ax1.set_xlabel('Epochen')\n",
    "ax1.set_ylabel('Verlust')\n",
    "ax1.set_title('Verlust-Lernkurve')\n",
    "ax1.legend()\n",
    "\n",
    "# Zweiter Plot: F1-Score-Lernkurve\n",
    "ax2.plot(val_f1_scores, label='Validierungs-F1-Score', color='orange')\n",
    "ax2.set_xlabel('Epochen')\n",
    "ax2.set_ylabel('F1-Score')\n",
    "ax2.set_title('F1-Score-Lernkurve')\n",
    "ax2.legend()\n",
    "\n",
    "# Die Plots anzeigen\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "Io33huiLgibB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711136977362,
     "user_tz": -60,
     "elapsed": 749,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "9728d992-d83a-4f7f-f428-f6b2083f5f38"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# Google Drive einbinden\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Basispfad festlegen (bereits gegeben)\n",
    "base_path = '/content/drive/My Drive/'\n",
    "\n",
    "# Datenpfad erstellen, falls er nicht existiert\n",
    "data_path = os.path.join(base_path, 'NLP/hate_speech_detection_pipeline/model') #NLP/hate_speech_detection_pipeline/model/model2.pth\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# Dateiname für das gespeicherte Modell festlegen\n",
    "model_file_path = os.path.join(data_path, 'model2.pth')\n",
    "\n",
    "# Modellzustand speichern (stellen Sie sicher, dass 'model' definiert und trainiert wurde)\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "\n",
    "# Überprüfen, ob die Datei tatsächlich existiert\n",
    "if os.path.isfile(model_file_path):\n",
    "    print(f'Modell wurde erfolgreich unter {model_file_path} gespeichert.')\n",
    "else:\n",
    "    print('Fehler beim Speichern des Modells.')\n"
   ],
   "metadata": {
    "id": "srHHzERCW8Wp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711137418432,
     "user_tz": -60,
     "elapsed": 3859,
     "user": {
      "displayName": "Linus Stuhlmann",
      "userId": "07279173608860907824"
     }
    },
    "outputId": "ffe1dea1-2f96-42b3-efd2-f2394b94d1e2"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Modell wurde erfolgreich unter /content/drive/My Drive/NLP/classification_pipeline/model/model2.pth gespeichert.\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "colab": {
   "provenance": [],
   "gpuType": "V100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
