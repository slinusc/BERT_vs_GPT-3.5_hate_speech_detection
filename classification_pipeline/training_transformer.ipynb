{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:02.212565Z",
     "start_time": "2024-03-21T13:48:01.522779Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if os.path.exists('../data/train/tokenized_train_data.pkl'):\n",
    "    data = pd.read_pickle('../data/train/tokenized_train_data.pkl')\n",
    "else:\n",
    "    print(\"File not found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:30.068262Z",
     "start_time": "2024-03-21T13:48:02.888021Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "If working in Google Colab:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_path = '/content/drive/My Drive/'\n",
    "data_path = os.path.join(base_path, 'NLP/data/train/tokenized_train_data.pkl')\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    data = pd.read_pickle(data_path)\n",
    "else:\n",
    "    print(\"File not found. Please ensure the file path is correct and run the previous cell to create the file.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:40.809403Z",
     "start_time": "2024-03-21T13:48:40.800549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                        comment_text  hate  \\\n0  Explanation Why the edits made under my userna...     0   \n1  Daww He matches this background colour Im seem...     0   \n2  Hey man Im really not trying to edit war Its j...     0   \n3   More I cant make any real suggestions on impr...     0   \n4  You sir are my hero Any chance you remember wh...     0   \n\n                                     tokenized  \n0  [input_ids, token_type_ids, attention_mask]  \n1  [input_ids, token_type_ids, attention_mask]  \n2  [input_ids, token_type_ids, attention_mask]  \n3  [input_ids, token_type_ids, attention_mask]  \n4  [input_ids, token_type_ids, attention_mask]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>hate</th>\n      <th>tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation Why the edits made under my userna...</td>\n      <td>0</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Daww He matches this background colour Im seem...</td>\n      <td>0</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man Im really not trying to edit war Its j...</td>\n      <td>0</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>More I cant make any real suggestions on impr...</td>\n      <td>0</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You sir are my hero Any chance you remember wh...</td>\n      <td>0</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initalize BERT Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:45.054259Z",
     "start_time": "2024-03-21T13:48:43.459376Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:47.798275Z",
     "start_time": "2024-03-21T13:48:47.600170Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, we need to drop any rows that failed to tokenize\n",
    "data = data.dropna(subset=['tokenized'])\n",
    "subset_train = data.iloc[:1000]\n",
    "\n",
    "# Extract 'input_ids' and 'attention_mask' and create tensors\n",
    "input_ids = torch.cat(subset_train['tokenized'].apply(lambda x: x['input_ids']).tolist())\n",
    "attention_mask = torch.cat(subset_train['tokenized'].apply(lambda x: x['attention_mask']).tolist())\n",
    "labels = torch.tensor(subset_train['hate'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if CUDA is available, otherwise use CPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for training\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:50.600905Z",
     "start_time": "2024-03-21T13:48:50.596401Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a DataLoader for the training and validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `input_ids`, `attention_mask`, and `labels` are your dataset's features\n",
    "# and labels, respectively, and are all PyTorch tensors.\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T13:48:57.444297Z",
     "start_time": "2024-03-21T13:48:57.438683Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the model and optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the number of epochs for training\n",
    "num_epochs = 3\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = model.to(device)  # Move the model to the appropriate device (CPU or GPU)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  33%|███▎      | 19/57 [10:49<21:33, 34.03s/it]"
     ]
    }
   ],
   "source": [
    "# Listen zur Speicherung der Verlustwerte\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)  # Move the batch to the appropriate device\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_loss_values.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            batch_predictions = np.argmax(logits, axis=1).flatten()\n",
    "            val_f1_scores.append(f1_score(label_ids.flatten(), batch_predictions))\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_loss_values.append(avg_val_loss)\n",
    "\n",
    "# Plotting the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss_values, label='Trainingsverlust')\n",
    "plt.plot(val_loss_values, label='Validierungsverlust')\n",
    "plt.xlabel('Epochen')\n",
    "plt.ylabel('Verlust')\n",
    "plt.title('Lernkurve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Displaying the average F1 score for validation set\n",
    "average_val_f1_score = np.mean(val_f1_scores)\n",
    "print(f\"Average F1 score on the validation set: {average_val_f1_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-21T13:49:04.573367Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
